{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming Data Loading Demonstration\n",
    "\n",
    "This notebook demonstrates the new streaming data loading capabilities added to the Pulse retrodiction training pipeline. The optimizations focus on three key areas:\n",
    "\n",
    "1. **Streaming Data Loading**: Process data as it's being loaded rather than loading all at once\n",
    "2. **Columnar Data Format Optimization**: Leverage Arrow/Parquet for high-performance analytical processing\n",
    "3. **Advanced Caching & Prefetching**: Intelligently preload and cache data based on access patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Add parent directory to path so we can import from recursive_training\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Sample Dataset\n",
    "\n",
    "First, let's create a sample dataset to work with. We'll create a large time series dataset to demonstrate the benefits of streaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create a sample dataset\n",
    "def create_sample_dataset(rows=100000, variables=5):\n",
    "    \"\"\"Create a sample dataset for testing\"\"\"\n",
    "    # Create a time series index\n",
    "    start_date = datetime(2020, 1, 1)\n",
    "    dates = [start_date + timedelta(hours=i) for i in range(rows)]\n",
    "    \n",
    "    # Create a DataFrame with random values\n",
    "    data = {}\n",
    "    data['timestamp'] = dates\n",
    "    \n",
    "    # Add some variables\n",
    "    for i in range(variables):\n",
    "        # Create a random walk with some seasonality\n",
    "        values = np.random.randn(rows).cumsum()\n",
    "        # Add some seasonality\n",
    "        values += 10 * np.sin(np.arange(rows) * 2 * np.pi / (24 * 7))  # Weekly pattern\n",
    "        data[f'variable_{i}'] = values\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "# Create a large sample dataset\n",
    "print(\"Creating sample dataset...\")\n",
    "large_df = create_sample_dataset(rows=100000, variables=5)\n",
    "print(f\"Created dataset with {len(large_df)} rows and {len(large_df.columns)} columns\")\n",
    "large_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store Data Using Different Data Stores\n",
    "\n",
    "Now, let's store the data using our different data store implementations to compare their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from recursive_training.data.data_store import RecursiveDataStore\n",
    "\n",
    "try:\n",
    "    from recursive_training.data.optimized_data_store import OptimizedDataStore\n",
    "    optimized_available = True\n",
    "except ImportError:\n",
    "    optimized_available = False\n",
    "    print(\"OptimizedDataStore not available\")\n",
    "\n",
    "try:\n",
    "    from recursive_training.data.streaming_data_store import StreamingDataStore\n",
    "    streaming_available = True\n",
    "except ImportError:\n",
    "    streaming_available = False\n",
    "    print(\"StreamingDataStore not available\")\n",
    "\n",
    "# Initialize data stores\n",
    "base_store = RecursiveDataStore({\"storage_path\": \"./demo_data/base\"})\n",
    "\n",
    "if optimized_available:\n",
    "    optimized_store = OptimizedDataStore({\"storage_path\": \"./demo_data/optimized\"})\n",
    "\n",
    "if streaming_available:\n",
    "    streaming_store = StreamingDataStore({\n",
    "        \"storage_path\": \"./demo_data/streaming\", \n",
    "        \"chunk_size\": 5000,\n",
    "        \"prefetch_chunks\": 2\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store Data in Each Format\n",
    "\n",
    "Now let's store our sample dataset using each data store implementation and measure the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Convert DataFrame to list of dictionaries for the data store\n",
    "print(\"Converting dataset to records...\")\n",
    "data_records = large_df.to_dict('records')\n",
    "print(f\"Converted to {len(data_records)} records\")\n",
    "\n",
    "# Store using base RecursiveDataStore\n",
    "print(\"\\nStoring data using base RecursiveDataStore...\")\n",
    "start_time = time.time()\n",
    "base_id = base_store.store_dataset(\"sample_data\", data_records)\n",
    "base_time = time.time() - start_time\n",
    "print(f\"Stored dataset with ID: {base_id} in {base_time:.2f} seconds\")\n",
    "\n",
    "# Store using OptimizedDataStore if available\n",
    "if optimized_available:\n",
    "    print(\"\\nStoring data using OptimizedDataStore...\")\n",
    "    start_time = time.time()\n",
    "    optimized_id = optimized_store.store_dataset_optimized(\"sample_data\", data_records)\n",
    "    optimized_time = time.time() - start_time\n",
    "    print(f\"Stored dataset with ID: {optimized_id} in {optimized_time:.2f} seconds\")\n",
    "    print(f\"Speedup vs base: {base_time / optimized_time:.2f}x\")\n",
    "else:\n",
    "    optimized_time = float('inf')\n",
    "\n",
    "# Store using StreamingDataStore if available\n",
    "if streaming_available:\n",
    "    print(\"\\nStoring data using StreamingDataStore...\")\n",
    "    \n",
    "    # Create a generator function to simulate streaming data\n",
    "    def stream_generator():\n",
    "        for record in data_records:\n",
    "            yield record\n",
    "    \n",
    "    start_time = time.time()\n",
    "    streaming_id = streaming_store.store_dataset_streaming(\"sample_data\", stream_generator(), batch_size=5000)\n",
    "    streaming_time = time.time() - start_time\n",
    "    print(f\"Stored dataset with ID: {streaming_id} in {streaming_time:.2f} seconds\")\n",
    "    print(f\"Speedup vs base: {base_time / streaming_time:.2f}x\")\n",
    "    print(f\"Speedup vs optimized: {optimized_time / streaming_time:.2f}x\")\n",
    "else:\n",
    "    streaming_time = float('inf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Retrieval Performance Comparison\n",
    "\n",
    "Now, let's compare the performance of retrieving data using the different data store implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Define a date range for filtering\n",
    "start_date = datetime(2020, 1, 15).isoformat()\n",
    "end_date = datetime(2020, 1, 20).isoformat()\n",
    "\n",
    "# Retrieve from base RecursiveDataStore\n",
    "print(\"\\nRetrieving data using base RecursiveDataStore...\")\n",
    "start_time = time.time()\n",
    "base_data, base_meta = base_store.retrieve_dataset(\"sample_data\")\n",
    "# Filter manually\n",
    "base_data_filtered = [item for item in base_data if start_date <= item.get('timestamp') <= end_date]\n",
    "base_time = time.time() - start_time\n",
    "print(f\"Retrieved {len(base_data)} records (filtered to {len(base_data_filtered)}) in {base_time:.2f} seconds\")\n",
    "\n",
    "# Retrieve from OptimizedDataStore if available\n",
    "if optimized_available:\n",
    "    print(\"\\nRetrieving data using OptimizedDataStore...\")\n",
    "    start_time = time.time()\n",
    "    optimized_df, optimized_meta = optimized_store.retrieve_dataset_optimized(\"sample_data\", start_time=start_date, end_time=end_date)\n",
    "    optimized_time = time.time() - start_time\n",
    "    print(f\"Retrieved {len(optimized_df)} filtered records in {optimized_time:.2f} seconds\")\n",
    "    print(f\"Speedup vs base: {base_time / optimized_time:.2f}x\")\n",
    "else:\n",
    "    optimized_time = float('inf')\n",
    "\n",
    "# Retrieve from StreamingDataStore if available\n",
    "if streaming_available:\n",
    "    print(\"\\nRetrieving data using StreamingDataStore streaming...\")\n",
    "    \n",
    "    # Define a callback function to process chunks\n",
    "    chunks_processed = 0\n",
    "    total_records = 0\n",
    "    \n",
    "    def process_chunk(chunk):\n",
    "        nonlocal chunks_processed, total_records\n",
    "        chunks_processed += 1\n",
    "        total_records += len(chunk)\n",
    "        # Just do some minimal processing to simulate real-world usage\n",
    "        mean_values = chunk.mean(numeric_only=True)\n",
    "        return mean_values\n",
    "    \n",
    "    start_time = time.time()\n",
    "    streaming_meta = streaming_store.retrieve_dataset_streaming(\n",
    "        \"sample_data\", process_chunk, start_time=start_date, end_time=end_date\n",
    "    )\n",
    "    streaming_time = time.time() - start_time\n",
    "    print(f\"Processed {chunks_processed} chunks with {total_records} records in {streaming_time:.2f} seconds\")\n",
    "    print(f\"Speedup vs base: {base_time / streaming_time:.2f}x\")\n",
    "    print(f\"Speedup vs optimized: {optimized_time / streaming_time:.2f}x\")\n",
    "    \n",
    "    # Also test the generator-based streaming interface\n",
    "    print(\"\\nRetrieving data using StreamingDataStore generator...\")\n",
    "    start_time = time.time()\n",
    "    total_records = 0\n",
    "    for chunk in streaming_store.stream_dataset(\"sample_data\", start_time=start_date, end_time=end_date):\n",
    "        total_records += len(chunk)\n",
    "        # Just do some minimal processing to simulate real-world usage\n",
    "        mean_values = chunk.mean(numeric_only=True)\n",
    "    \n",
    "    gen_streaming_time = time.time() - start_time\n",
    "    print(f\"Processed {total_records} records using generator in {gen_streaming_time:.2f} seconds\")\n",
    "else:\n",
    "    streaming_time = float('inf')\n",
    "    gen_streaming_time = float('inf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Usage Comparison\n",
    "\n",
    "One of the main advantages of streaming data is reduced memory usage. Let's compare the memory usage of the different approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import psutil\n",
    "import gc\n",
    "\n",
    "def get_memory_usage():\n",
    "    \"\"\"Get current memory usage in MB\"\"\"\n",
    "    return psutil.Process(os.getpid()).memory_info().rss / (1024 * 1024)\n",
    "\n",
    "# Function to measure memory usage during data retrieval\n",
    "def measure_memory_usage(retrieval_func, *args, **kwargs):\n",
    "    # Force garbage collection to get a clean measurement\n",
    "    gc.collect()\n",
    "    \n",
    "    # Measure memory before\n",
    "    start_memory = get_memory_usage()\n",
    "    \n",
    "    # Perform the retrieval\n",
    "    result = retrieval_func(*args, **kwargs)\n",
    "    \n",
    "    # Measure peak memory during/after retrieval\n",
    "    peak_memory = get_memory_usage()\n",
    "    \n",
    "    # Calculate memory usage\n",
    "    memory_used = peak_memory - start_memory\n",
    "    \n",
    "    return result, memory_used\n",
    "\n",
    "# Compare memory usage for different retrieval methods\n",
    "print(\"\\nComparing memory usage for different retrieval methods...\")\n",
    "\n",
    "# Base data store\n",
    "results, base_memory = measure_memory_usage(\n",
    "    lambda: base_store.retrieve_dataset(\"sample_data\")\n",
    ")\n",
    "print(f\"Base DataStore memory usage: {base_memory:.2f} MB\")\n",
    "\n",
    "# Optimized data store\n",
    "if optimized_available:\n",
    "    results, optimized_memory = measure_memory_usage(\n",
    "        lambda: optimized_store.retrieve_dataset_optimized(\"sample_data\")\n",
    "    )\n",
    "    print(f\"OptimizedDataStore memory usage: {optimized_memory:.2f} MB\")\n",
    "    print(f\"Memory reduction vs base: {base_memory / optimized_memory:.2f}x\")\n",
    "else:\n",
    "    optimized_memory = float('inf')\n",
    "\n",
    "# Streaming data store with callback\n",
    "if streaming_available:\n",
    "    chunks_processed = 0\n",
    "    total_records = 0\n",
    "    \n",
    "    def process_chunk(chunk):\n",
    "        nonlocal chunks_processed, total_records\n",
    "        chunks_processed += 1\n",
    "        total_records += len(chunk)\n",
    "    \n",
    "    results, streaming_memory = measure_memory_usage(\n",
    "        lambda: streaming_store.retrieve_dataset_streaming(\"sample_data\", process_chunk)\n",
    "    )\n",
    "    print(f\"StreamingDataStore memory usage: {streaming_memory:.2f} MB\")\n",
    "    print(f\"Memory reduction vs base: {base_memory / streaming_memory:.2f}x\")\n",
    "    print(f\"Memory reduction vs optimized: {optimized_memory / streaming_memory:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance with PyArrow/Parquet\n",
    "\n",
    "One of the optimizations in `StreamingDataStore` is the use of columnar data formats like Parquet with PyArrow. Let's specifically test those capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Skip this cell if PyArrow isn't available\n",
    "if streaming_available:\n",
    "    try:\n",
    "        import pyarrow as pa\n",
    "        pyarrow_available = True\n",
    "    except ImportError:\n",
    "        pyarrow_available = False\n",
    "        print(\"PyArrow not available - skipping Arrow-specific tests\")\n",
    "else:\n",
    "    pyarrow_available = False\n",
    "    \n",
    "if streaming_available and pyarrow_available:\n",
    "    print(\"\\nTesting PyArrow-specific capabilities...\")\n",
    "    \n",
    "    # Test the Arrow-specific streaming method\n",
    "    start_time = time.time()\n",
    "    total_records = 0\n",
    "    total_batches = 0\n",
    "    \n",
    "    # Use the Arrow RecordBatch generator\n",
    "    for batch in streaming_store.stream_dataset_arrow(\"sample_data\", start_time=start_date, end_time=end_date):\n",
    "        total_records += batch.num_rows\n",
    "        total_batches += 1\n",
    "        \n",
    "        # Do some Arrow-specific processing\n",
    "        # Calculate the mean of one of the numeric columns using PyArrow compute\n",
    "        if 'variable_0' in batch.schema.names:\n",
    "            mean = pa.compute.mean(batch.column('variable_0')).as_py()\n",
    "    \n",
    "    arrow_time = time.time() - start_time\n",
    "    print(f\"Processed {total_records} records in {total_batches} Arrow batches in {arrow_time:.2f} seconds\")\n",
    "    \n",
    "    # Compare with Pandas-based processing from earlier\n",
    "    if 'gen_streaming_time' in locals():\n",
    "        print(f\"Speedup vs Pandas-based streaming: {gen_streaming_time / arrow_time:.2f}x\")\n",
    "    \n",
    "    # Also test creating data with the Arrow interface\n",
    "    # Create a smaller dataset for this test\n",
    "    small_df = create_sample_dataset(rows=10000, variables=3)\n",
    "    \n",
    "    print(\"\\nCreating dataset using Arrow Table interface...\")\n",
    "    start_time = time.time()\n",
    "    dataset_id, arrow_table = streaming_store.create_arrow_table(\"arrow_sample\", small_df.to_dict('records'))\n",
    "    arrow_create_time = time.time() - start_time\n",
    "    print(f\"Created Arrow table with {arrow_table.num_rows} rows in {arrow_create_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integration with ParallelTrainingCoordinator\n",
    "\n",
    "Finally, let's demonstrate the integration with the ParallelTrainingCoordinator to show how these optimizations benefit the overall training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from recursive_training.parallel_trainer import ParallelTrainingCoordinator\n",
    "from datetime import datetime\n",
    "\n",
    "# Initialize the coordinator\n",
    "coordinator = ParallelTrainingCoordinator(max_workers=2)\n",
    "\n",
    "# Create some dummy variables that match our created datasets\n",
    "variables = ['variable_0', 'variable_1', 'variable_2']\n",
    "\n",
    "# Choose a time range that has data\n",
    "start_time = datetime(2020, 1, 1)\n",
    "end_time = datetime(2020, 1, 10)\n",
    "\n",
    "# Prepare batches for training\n",
    "print(\"\\nPreparing training batches...\")\n",
    "batches = coordinator.prepare_training_batches(\n",
    "    variables=variables,\n",
    "    start_time=start_time,\n",
    "    end_time=end_time,\n",
    "    batch_size_days=2,  # Smaller batch size for demonstration\n",
    "    overlap_days=1,     # Some overlap between batches\n",
    "    preload_data=True   # Enable data preloading\n",
    ")\n",
    "print(f\"Created {len(batches)} training batches\")\n",
    "\n",
    "# Define a simple progress callback\n",
    "def progress_callback(progress_data):\n",
    "    print(f\"Progress: {progress_data['completed_percentage']} ({progress_data['completed_batches']}/{progress_data['total_batches']} batches)\")\n",
    "\n",
    "# Start training (this will use our streaming data store if available)\n",
    "print(\"\\nStarting parallel training...\")\n",
    "coordinator.start_training(progress_callback=progress_callback)\n",
    "\n",
    "# Get the results\n",
    "results = coordinator.get_results_summary()\n",
    "print(\"\\nTraining completed with results:\")\n",
    "print(f\"Speedup factor: {results['performance']['speedup_factor']:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This demonstration has shown the benefits of the streaming data loading optimizations:\n",
    "\n",
    "1. **Memory Efficiency**: Streaming data processing uses significantly less memory compared to loading entire datasets at once.\n",
    "2. **Performance**: The optimized columnar data formats with Arrow/Parquet provide better performance for analytical workloads.\n",
    "3. **Scalability**: These optimizations make it possible to work with larger datasets that wouldn't fit in memory otherwise.\n",
    "4. **Integration**: The streaming data capabilities integrate seamlessly with the parallel training framework."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}