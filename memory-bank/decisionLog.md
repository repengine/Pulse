# Decision Log

This file records architectural and implementation decisions using a list format.
2025-04-30 02:03:01 - Log of updates made.

*

## Decision

* [2025-04-30 12:32:55] - **Decision:** Implement robust type checking and conversion in the forecast ensemble module to prevent "must be real number, not str" errors.
* [2025-04-30 20:45:30] - **Decision:** Implement a hybrid rules approach for recursive AI training that maintains compatibility with existing dictionary-based rules while gradually introducing object capabilities.
* [2025-04-30 20:47:55] - **Decision:** Update Pulse to version 0.3.0 "Recursive Learning" to reflect the new recursive AI training implementation.
* [2025-04-30 21:41:15] - **Decision:** Implement RecursiveRuleGenerator with a GPT-Symbolic feedback loop for Phase 2 of the Recursive AI Training system.
* [2025-04-30 21:41:30] - **Decision:** Implement RecursiveRuleEvaluator as a separate component for rule effectiveness testing.
* [2025-04-30 21:41:45] - **Decision:** Implement RuleRepository with versioning and querying capabilities to manage rule lifecycle.
* [2025-05-01 00:11:00] - **Decision:** Implement uncertainty-driven curriculum for retrodiction in the Recursive AI Training system.
* [2025-05-01 00:11:15] - **Decision:** Create a comprehensive error handling system with recovery mechanisms for the Recursive AI Training system.
* [2025-05-01 00:11:30] - **Decision:** Implement visualization tools for metrics monitoring and analysis in the Recursive AI Training system.
* [2025-05-01 20:35:00] - **Decision:** Implement consistent data persistence during ingestion across all API plugins using the iris_utils.ingestion_persistence module.
* [2025-05-02 10:43:00] - **Decision:** Adopt a hybrid RAG + lightweight domain-adapter approach for the Pulse Conversational Interface to mitigate token costs and improve efficiency, based on the refined proposal from the `Think` mode. Initial technology choices include Sentence-Transformers and Faiss for RAG, and preparing for integration of an open-source 7B LLM with LoRA using libraries like Hugging Face Transformers and PEFT. A basic Tkinter UI will be used initially.
* [2025-05-02 11:47:00] - **Decision:** Leverage Alpha Vantage premium API access (150 requests/min with 15-minute delayed and real-time US stock market data) to enhance Pulse's data ingestion capabilities and expand its predictive models with more current market data.
* [2025-05-02 12:20:00] - **Decision:** Enhance API plugins to write data to file incrementally during ingestion, not just after completion, to prevent data loss and make real-time data immediately available.

## Rationale

* [2025-04-30 12:32:55] - **Rationale:** The forecasting pipeline was experiencing errors when string values were encountered in places where numeric operations were being performed. This was happening because some forecasts or adjustments were being stored as strings rather than numbers, likely during serialization/deserialization processes or data extraction from external sources. Explicit type conversion with error handling was needed to ensure robust operation.
* [2025-04-30 20:45:30] - **Rationale:** A hybrid rules approach provides a smooth transition path that preserves compatibility with existing systems while enabling gradual enhancement of rule capabilities. This approach minimizes disruption to current operations while allowing us to incrementally introduce more sophisticated rule structures, improved type safety, and object-oriented features. The transition will happen in parallel with the phased implementation of recursive AI training capabilities.
* [2025-04-30 20:47:55] - **Rationale:** The implementation of recursive AI training represents a significant enhancement to the system capabilities, warranting a minor version update from v0.2.1 to v0.3.0. The "Recursive Learning" subtitle highlights the central new feature of this release.
* [2025-04-30 21:41:15] - **Rationale:** Implementing a GPT-Symbolic feedback loop in the RecursiveRuleGenerator allows for continuous improvement of rule quality by leveraging both LLM capabilities and symbolic verification. This approach facilitates more effective rule generation through the combination of GPT's creative generation abilities with symbolic system feedback for validation and refinement.
* [2025-04-30 21:41:30] - **Rationale:** A dedicated RecursiveRuleEvaluator component allows for systematic assessment of rule effectiveness outside of the generation process. This separation of concerns enables more thorough testing of rules against multiple datasets and conditions, providing clear metrics for rule quality and identifying areas for improvement.
* [2025-04-30 21:41:45] - **Rationale:** A versioned RuleRepository with querying capabilities enables efficient rule management throughout their lifecycle. This approach supports historical analysis, A/B testing of rule sets, and graceful evolution of rules over time while maintaining system stability.
* [2025-05-01 00:11:00] - **Rationale:** Implementing an uncertainty-driven curriculum for retrodiction allows the system to focus training resources on the most challenging or uncertain predictions. This approach optimizes learning efficiency by prioritizing areas where the system has the lowest confidence, leading to faster improvement in overall prediction accuracy and robustness.
* [2025-05-01 00:11:15] - **Rationale:** A comprehensive error handling system is essential for maintaining system stability during recursive training cycles. By implementing robust error detection, logging, and recovery mechanisms, the system can gracefully handle failures without interrupting the training process, ensuring data integrity and continuous operation even when unexpected conditions arise.
* [2025-05-01 00:11:30] - **Rationale:** Visualization tools for metrics provide critical insights into system performance and behavior during recursive training. These tools enable rapid identification of trends, anomalies, and optimization opportunities that might not be apparent from raw data alone, facilitating more effective monitoring, debugging, and improvement of the recursive training process.
* [2025-05-01 20:35:00] - **Rationale:** Storing data at each step of the ingestion process (request metadata, API responses, and processed signals) provides significant benefits: better debugging, comprehensive historical records, and improved recovery from failures. Standardizing this approach across all plugins ensures consistent behavior and simplifies maintenance.
* [2025-05-02 11:47:00] - **Rationale:** Premium Alpha Vantage access provides several key advantages: (1) Higher API rate limits allow for more comprehensive and frequent data collection; (2) Access to both delayed and real-time US stock market data enables more accurate current market analysis; (3) The ability to retrieve intraday data with various intervals provides finer-grained temporal resolution for models; (4) The combination with our recently implemented data persistence capabilities ensures all retrieved market data is properly stored for historical analysis and model training.
* [2025-05-02 12:20:00] - **Rationale:** Writing data to file incrementally during ingestion provides several benefits: (1) Data is preserved even if the ingestion process is interrupted, preventing data loss; (2) Real-time data becomes immediately available to other processes without waiting for the entire ingestion to complete; (3) System becomes more robust against crashes or API failures; (4) Creates a more accurate historical record of when data was acquired; (5) Improves traceability and debugging by recording each step of the data ingestion process.

## Implementation Details

* [2025-04-30 04:10:05] - Phase 2 Orchestrator Refactoring: TrustEngine refactored for SRP and Strategy Pattern (enrichment and scoring logic extracted to services, symbolic tag logic prepared for service extraction). Simulator Core refactored to use Command Pattern and SimulationRunner abstraction. All usages updated to new interfaces.
* [2025-04-30 12:32:55] - **Implementation Details:** Modified forecast_ensemble.py to add explicit float() conversions with proper try/except blocks to handle conversion errors gracefully. Added appropriate warning logs to identify invalid values at runtime. Used safe default values (0.0) when conversion fails to allow the system to continue functioning despite bad input data.
* [2025-04-30 20:45:30] - **Implementation Details:** The implementation plan includes: (1) Creating adapter interfaces that work with both dictionary and object-based rules; (2) Implementing cost control systems with token/request limits to prevent unexpected expenses; (3) Building the recursive AI training pipeline in phases (Data Pipeline and Core Metrics in Phase 1, Rule Generation in Phase 2, and Advanced Metrics + Error Handling in Phase 3); (4) Incorporating GPT-Symbolic feedback loops and uncertainty-driven curriculum for more efficient training; (5) Developing a rule review dashboard for monitoring and managing rule quality and performance.
* [2025-04-30 20:47:55] - **Implementation Details:** Updated the README.md to reflect version 0.3.0 "Recursive Learning", added a new patch notes section for v0.3.x detailing the key features of the recursive AI training implementation, and preserved all existing content from the previous version of the README.
* [2025-04-30 21:41:15] - **Implementation Details:** The RecursiveRuleGenerator was implemented with a multi-stage pipeline: (1) Initial rule generation using GPT with targeted prompts; (2) Symbolic verification of generated rules against test datasets; (3) Feedback loop that incorporates verification results to refine rules; (4) Threshold-based acceptance criteria to ensure only high-quality rules are promoted to the repository. The implementation includes token usage tracking and rate limiting to control costs.
* [2025-04-30 21:41:30] - **Implementation Details:** The RecursiveRuleEvaluator was implemented with comprehensive metrics for rule assessment: precision, recall, F1 score, and custom domain-specific metrics. The evaluator includes capabilities for batch evaluation of multiple rules, comparative analysis against baseline rule sets, and detailed reporting of evaluation results. Integration with the symbolic system ensures consistent verification methods between generation and evaluation phases.
* [2025-04-30 21:41:45] - **Implementation Details:** The RuleRepository was implemented with: (1) A versioning system that maintains the full history of rule evolution; (2) Metadata tagging for efficient rule categorization and retrieval; (3) Query capabilities based on performance metrics, domains, and usage patterns; (4) Export/import functionality for rule sharing across environments; (5) Integration with the hybrid dictionary/object adapter to ensure compatibility with both representation formats.
* [2025-05-01 00:11:00] - **Implementation Details:** The uncertainty-driven curriculum for retrodiction was implemented with: (1) Bayesian uncertainty estimation for each prediction; (2) Dynamic prioritization algorithm that schedules training examples based on uncertainty levels; (3) Adaptive difficulty scaling to gradually increase complexity as performance improves; (4) Integration with the metrics system to track uncertainty reduction over training cycles; (5) Checkpoint mechanism to save curriculum state for resuming training sessions.
* [2025-05-01 00:11:15] - **Implementation Details:** The comprehensive error handling system was implemented with: (1) Hierarchical error classification framework with domain-specific error types; (2) Centralized error logging with contextual metadata capture; (3) Automatic recovery strategies for common failure modes; (4) Graceful degradation paths for critical components; (5) Circuit breaker pattern implementation to prevent cascading failures; (6) Comprehensive error reporting dashboard for monitoring and analysis.
* [2025-05-01 00:11:30] - **Implementation Details:** The visualization tools for metrics were implemented with: (1) Real-time performance dashboards with configurable metrics display; (2) Comparative visualization capabilities for A/B testing rule sets; (3) Time-series analysis tools for tracking performance evolution; (4) Uncertainty visualization through confidence intervals and heatmaps; (5) Exportable reports for offline analysis; (6) Integration with the error handling system to correlate errors with performance metrics.
* [2025-05-01 20:35:00] - **Implementation Details:** Updated multiple plugins including NASDAQ, Google Trends, and OpenAQ to use the shared iris_utils.ingestion_persistence module. Identified opportunities for further abstraction through wrappers, decorators, or mixins to reduce code duplication in the future.
* [2025-05-02 11:47:00] - **Implementation Details:** The implementation plan for leveraging premium Alpha Vantage access includes: (1) Enhancing the AlphaVantagePlugin to support both delayed and real-time data entitlements; (2) Implementing configuration options to control data freshness requirements and manage API quota usage; (3) Expanding the supported symbols and datasets to include more US equities and technical indicators; (4) Leveraging the recently implemented data persistence capabilities to store all collected market data; (5) Adding new market-related variables to the VariableRegistry; (6) Developing enhanced predictive models that can benefit from higher frequency data; (7) Implementing system monitoring to track API usage against the 150 requests/minute limit.
* [2025-05-02 12:20:00] - **Implementation Details:** The implementation included: (1) Created a new `save_data_point_incremental` function in `ingestion_persistence.py` that writes individual data points to JSONL files immediately upon acquisition; (2) Modified AlphaVantagePlugin to use this function for all data types (stocks, forex, crypto, and economic indicators); (3) Updated NASDAQ plugin to use incremental saving during both regular ingestion and mock data generation; (4) Created a test script `test_incremental_ingestion.py` to verify that data is written in real-time; (5) Maintained backward compatibility with original `save_processed_data` function to ensure existing code continues to work. This approach provides immediate data availability while preserving the existing ingestion workflow.
* [2025-05-02 09:55:00] - **Decision:** Build a comprehensive 5-year historical timeline for Pulse variables to support historical retrodiction training.
* [2025-05-02 09:55:00] - **Rationale:** A verified and complete historical dataset is crucial for training and evaluating retrodiction models, ensuring the reliability and accuracy of Pulse's forecasting capabilities. Standardizing the data pipeline improves efficiency, maintainability, and data quality.
* [2025-05-02 09:55:00] - **Implementation Details:** Implemented a multi-phase project covering variable identification, data retrieval with persistence, standardized storage in RecursiveDataStore, comprehensive verification (completeness, consistency, anomalies, cross-source), and systematic handling of missing data and inconsistencies. Developed dedicated utility modules and a CLI for managing the pipeline. Emphasized verification ('Rarely trust, Always verify') throughout all phases.