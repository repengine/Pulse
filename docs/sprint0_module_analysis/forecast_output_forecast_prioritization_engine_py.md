# Analysis Report: `forecast_output/forecast_prioritization_engine.py`

## 1. Module Intent/Purpose

The primary role of the [`forecast_output/forecast_prioritization_engine.py`](forecast_output/forecast_prioritization_engine.py:1) module is to rank "certified" forecasts. This ranking is intended for various uses, such as operator review, data export, or strategic decision-making. The prioritization is based on a combination of factors including forecast alignment, confidence, and a predefined symbolic priority associated with different "arcs" (narrative themes or scenarios).

## 2. Operational Status/Completeness

The module appears to be functionally complete for its defined scope (ranking and exporting forecasts based on specific criteria).
- It is versioned as `v1.0.0` in its docstring, suggesting a stable release.
- It includes an inline basic test function, [`_test_forecast_prioritization_engine()`](forecast_output/forecast_prioritization_engine.py:97), which indicates a level of self-contained validation.
- No explicit `TODO` comments or obvious placeholders for core functionality were observed.

## 3. Implementation Gaps / Unfinished Next Steps

*   **"Trust" Factor Mismatch:** The module's docstring (lines 6-7) states that ranking is based on "alignment, confidence, arc symbolic priority, and **trust**." However, the actual implementation in [`rank_certified_forecasts()`](forecast_output/forecast_prioritization_engine.py:44) sorts forecasts using `alignment_score`, `confidence`, and `arc_weight` (derived from `arc_label`), but does **not** incorporate a "trust" metric. This is a potential gap or an outdated comment.
*   **Configurable Arc Priorities:** The [`ARC_PRIORITIES`](forecast_output/forecast_prioritization_engine.py:18) dictionary is hardcoded. For greater flexibility and maintainability, these priorities could be loaded from an external configuration file or database.
*   **Advanced Error Handling:** The error handling in [`export_prioritized_forecasts()`](forecast_output/forecast_prioritization_engine.py:77) is basic, logging errors to the console. More robust mechanisms (e.g., custom exceptions, retry logic) could be beneficial.
*   **Integration with UI/Strategic Systems:** While the module can export ranked forecasts, direct integration points for feeding these into a UI or other strategic systems are not part of this module itself and would be logical next steps in a broader system.
*   **Extensibility of Ranking Criteria:** The ranking logic is fixed. Future enhancements might involve allowing more dynamic or configurable ranking strategies.

## 4. Connections & Dependencies

*   **Direct Imports from Other Project Modules:** None observed in this file. It operates as a utility module processing data structures passed to it.
*   **External Library Dependencies:**
    *   [`json`](https://docs.python.org/3/library/json.html): Used in [`export_prioritized_forecasts()`](forecast_output/forecast_prioritization_engine.py:77) for serializing forecast dictionaries to a file.
    *   [`logging`](https://docs.python.org/3/library/logging.html): Used for logging informational and error messages.
    *   `typing` ([`List`](https://docs.python.org/3/library/typing.html#typing.List), [`Dict`](https://docs.python.org/3/library/typing.html#typing.Dict)): Used for type hinting.
*   **Interaction via Shared Data:**
    *   **Input:** Expects a list of forecast dictionaries. These dictionaries are assumed to be generated by upstream modules within the forecast generation pipeline. Key fields expected include `certified` (boolean), `alignment_score` (float), `confidence` (float), and `arc_label` (string).
    *   **Output:** Produces a ranked list of these forecast dictionaries. Can also write this ranked list to a JSONL file via [`export_prioritized_forecasts()`](forecast_output/forecast_prioritization_engine.py:77).
*   **Input/Output Files:**
    *   **Output:** The [`export_prioritized_forecasts()`](forecast_output/forecast_prioritization_engine.py:77) function writes ranked forecasts to a user-specified file path. Each forecast is a JSON object on a new line.

## 5. Function and Class Example Usages

This module contains functions, not classes.

*   **[`prioritize_by_arc_weight(forecast: Dict) -> int`](forecast_output/forecast_prioritization_engine.py:30)**
    *   Calculates a numerical score for a forecast based on its `arc_label` using the [`ARC_PRIORITIES`](forecast_output/forecast_prioritization_engine.py:18) mapping.
    ```python
    forecast_example = {"arc_label": "Hope Surge", "alignment_score": 0.9, "confidence": 0.8, "certified": True}
    arc_score = prioritize_by_arc_weight(forecast_example)
    # arc_score would be 3
    ```

*   **[`rank_certified_forecasts(forecasts: List[Dict]) -> List[Dict]`](forecast_output/forecast_prioritization_engine.py:44)**
    *   Filters for certified forecasts and then sorts them based on `alignment_score` (descending), `confidence` (descending), and the arc weight (descending).
    ```python
    forecast_list = [
        {"arc_label": "Hope Surge", "alignment_score": 0.9, "confidence": 0.8, "certified": True},
        {"arc_label": "Collapse Risk", "alignment_score": 0.7, "confidence": 0.6, "certified": True},
        {"arc_label": "Stabilization", "alignment_score": 0.9, "confidence": 0.7, "certified": True}
    ]
    ranked_forecasts = rank_certified_forecasts(forecast_list)
    # ranked_forecasts[0] would be the "Hope Surge" forecast.
    ```

*   **[`select_top_forecasts(forecasts: List[Dict], top_n: int = 10) -> List[Dict]`](forecast_output/forecast_prioritization_engine.py:62)**
    *   Takes a list of forecasts, ranks them using [`rank_certified_forecasts()`](forecast_output/forecast_prioritization_engine.py:44), and returns the top `top_n` forecasts.
    ```python
    # Using forecast_list from above
    top_2 = select_top_forecasts(forecast_list, top_n=2)
    # top_2 would contain the "Hope Surge" and "Stabilization" forecasts.
    ```

*   **[`export_prioritized_forecasts(forecasts: List[Dict], path: str)`](forecast_output/forecast_prioritization_engine.py:77)**
    *   Writes a list of forecasts to the specified file path in JSONL format.
    ```python
    # Using ranked_forecasts from above
    export_prioritized_forecasts(ranked_forecasts, "output/ranked_forecasts.jsonl")
    # This creates 'ranked_forecasts.jsonl' with each forecast as a JSON line.
    ```

## 6. Hardcoding Issues

*   **[`ARC_PRIORITIES`](forecast_output/forecast_prioritization_engine.py:18):** The dictionary mapping arc labels (e.g., "Hope Surge", "Collapse Risk") to numerical weights (e.g., 3, -2) is hardcoded. Changes require code modification.
*   **Default `top_n`:** In [`select_top_forecasts()`](forecast_output/forecast_prioritization_engine.py:62), the `top_n` parameter defaults to `10`.
*   **Default Scores for Sorting:** In [`rank_certified_forecasts()`](forecast_output/forecast_prioritization_engine.py:44), missing `alignment_score` or `confidence` in a forecast dictionary will default to `0` for sorting purposes. Similarly, an unknown `arc_label` in [`prioritize_by_arc_weight()`](forecast_output/forecast_prioritization_engine.py:30) defaults to an arc weight of `0`.
*   **Logger Name:** The logger is named `"forecast_prioritization_engine"` via [`logging.getLogger("forecast_prioritization_engine")`](forecast_output/forecast_prioritization_engine.py:26).

## 7. Coupling Points

*   **Forecast Data Structure:** The module is tightly coupled to the expected structure of the input forecast dictionaries. It relies on specific keys: `certified`, `alignment_score`, `confidence`, and `arc_label`. Changes to this data contract in upstream modules would break this engine.
*   **`ARC_PRIORITIES` Definition:** The core ranking logic is dependent on the specific string labels and their associated integer values defined in [`ARC_PRIORITIES`](forecast_output/forecast_prioritization_engine.py:18).
*   **Output File Format:** The [`export_prioritized_forecasts()`](forecast_output/forecast_prioritization_engine.py:77) function produces output in JSONL format. Downstream consumers are coupled to this specific format.

## 8. Existing Tests

*   **Inline Test Function:** The module includes a basic, self-contained test function: [`_test_forecast_prioritization_engine()`](forecast_output/forecast_prioritization_engine.py:97).
    *   This test uses a small, hardcoded list of dummy forecasts.
    *   It performs a simple assertion on the outcome of [`rank_certified_forecasts()`](forecast_output/forecast_prioritization_engine.py:44), checking if the top-ranked forecast matches expectations based on the dummy data and [`ARC_PRIORITIES`](forecast_output/forecast_prioritization_engine.py:18).
*   **Formal Test Suite:** Based on the provided file listing, there does not appear to be a dedicated test file (e.g., `tests/forecast_output/test_forecast_prioritization_engine.py` or similar in the main `tests/` directory) for this module.
*   **Coverage and Gaps:**
    *   The existing inline test provides minimal coverage.
    *   It does not test [`select_top_forecasts()`](forecast_output/forecast_prioritization_engine.py:62) or [`export_prioritized_forecasts()`](forecast_output/forecast_prioritization_engine.py:77) (exporting would typically involve mocking file system interactions).
    *   Edge cases such as empty input lists, forecasts missing essential keys, or lists with no certified forecasts are not covered by this basic test.

## 9. Module Architecture and Flow

The module follows a simple, sequential data processing flow:

1.  **Input:** Receives a `List[Dict]` representing forecasts.
2.  **Certification Filter:** The [`rank_certified_forecasts()`](forecast_output/forecast_prioritization_engine.py:44) function first filters this list to retain only forecasts where the `certified` key is `True`.
3.  **Arc Weighting:** For each certified forecast, [`prioritize_by_arc_weight()`](forecast_output/forecast_prioritization_engine.py:30) determines a numerical weight based on its `arc_label` by looking it up in the [`ARC_PRIORITIES`](forecast_output/forecast_prioritization_engine.py:18) dictionary.
4.  **Multi-Criteria Sorting:** [`rank_certified_forecasts()`](forecast_output/forecast_prioritization_engine.py:44) then sorts the certified forecasts. The sorting is performed in descending order based on a tuple of criteria:
    1.  `alignment_score` (primary key)
    2.  `confidence` (secondary key)
    3.  Arc weight (tertiary key, from step 3)
5.  **Top-N Selection (Optional):** The [`select_top_forecasts()`](forecast_output/forecast_prioritization_engine.py:62) function can be used to take the fully ranked list and truncate it to the `top_n` highest-ranked forecasts.
6.  **Export (Optional):** The [`export_prioritized_forecasts()`](forecast_output/forecast_prioritization_engine.py:77) function takes a list of forecasts (presumably already ranked) and writes them to a specified file path, with each forecast dictionary as a JSON string on a new line.

The overall architecture is a set of utility functions designed to be called in sequence to process and refine a list of forecasts.

## 10. Naming Conventions

*   **Functions:** Use `snake_case` (e.g., [`rank_certified_forecasts`](forecast_output/forecast_prioritization_engine.py:44), [`export_prioritized_forecasts`](forecast_output/forecast_prioritization_engine.py:77)), adhering to PEP 8.
*   **Variables:** Predominantly `snake_case` (e.g., `alignment_score`, `arc_label`, `certified_forecasts`). The loop variable `fc` in [`export_prioritized_forecasts()`](forecast_output/forecast_prioritization_engine.py:90) is short but understandable in context as "forecast".
*   **Constants:** Use `UPPER_SNAKE_CASE` (e.g., [`ARC_PRIORITIES`](forecast_output/forecast_prioritization_engine.py:18)), adhering to PEP 8.
*   **Module Name:** [`forecast_prioritization_engine.py`](forecast_output/forecast_prioritization_engine.py:1) is descriptive and follows `snake_case`.
*   **Internal Test Function:** [`_test_forecast_prioritization_engine()`](forecast_output/forecast_prioritization_engine.py:97) uses a leading underscore, correctly indicating it's intended for internal use/testing.
*   **Overall:** Naming conventions are consistent and align well with Python community standards (PEP 8). No significant deviations or potential AI assumption errors in naming were observed.