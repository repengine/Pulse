# Module Analysis: `recursive_training.advanced_metrics.visualization`

**Source File:** [`recursive_training/advanced_metrics/visualization.py`](../../recursive_training/advanced_metrics/visualization.py)
**Test File:** [`tests/recursive_training/advanced_metrics/test_visualization.py`](../../tests/recursive_training/advanced_metrics/test_visualization.py)

## 1. Module Intent/Purpose

The primary role of the [`visualization.py`](../../recursive_training/advanced_metrics/visualization.py) module is to provide plotting utilities and dashboards for visualizing advanced metrics generated during the recursive training process. This includes metrics related to model performance (MSE, accuracy), uncertainty, data drift, and convergence diagnostics, aiming to offer insights into the training dynamics.

## 2. Operational Status/Completeness

The module appears to be operational for its currently implemented features:
- Plotting a metrics dashboard ([`plot_metrics_dashboard()`](../../recursive_training/advanced_metrics/visualization.py:23)).
- Plotting a reliability diagram ([`plot_reliability_diagram()`](../../recursive_training/advanced_metrics/visualization.py:84)).

It includes checks for the availability of `matplotlib` and `seaborn`, gracefully degrading by printing messages if these libraries are not installed. No explicit `TODO` comments or `pass` statements indicating unfinished sections were found within the provided code.

## 3. Implementation Gaps / Unfinished Next Steps

- **Limited Plot Types:** The module currently offers only two specific visualization functions. It could be expanded to include other relevant plots for advanced metrics, such as:
    - Distribution plots for uncertainty scores.
    - More detailed visualizations for drift detection (e.g., feature-wise drift).
    - Specific plots for different aspects of convergence beyond a simple boolean status.
- **`seaborn` Not Utilized:** Although `seaborn` is checked for availability ([`SEABORN_AVAILABLE`](../../recursive_training/advanced_metrics/visualization.py:19)), it is not currently used in any of the plotting functions. This suggests an initial intention to use `seaborn` for potentially more aesthetically advanced or statistically-oriented plots, which has not yet been realized.
- **Input Data Structure Rigidity:** The functions expect specific nested dictionary structures for `metrics_history` and `calibration_metrics`. Adding more robust input validation or an adaptation layer could make the module more resilient to variations in input data.
- **Customization:** Plot customization options (e.g., colors, figure sizes, titles, output file paths) are largely hardcoded. Exposing these as parameters would increase flexibility.

## 4. Connections & Dependencies

-   **Direct Imports from Other Project Modules:** None are directly imported within this module. It functions as a utility that consumes data produced elsewhere.
-   **External Library Dependencies:**
    -   `logging` (Python standard library)
    -   `typing.Any`, `typing.Dict`, `typing.List`, `typing.Optional` (Python standard library)
    -   `matplotlib.pyplot` (optional, required for plotting)
    -   `seaborn` (optional, checked for availability but not currently used)
-   **Interaction with Other Modules via Shared Data:**
    -   This module is designed to consume data structures (lists of dictionaries, dictionaries) generated by other modules within the `recursive_training` package, particularly those responsible for metrics calculation (e.g., from `recursive_training.advanced_metrics` or `recursive_training.metrics`).
-   **Input/Output Files:**
    -   **Input:** The module takes Python data structures (lists and dictionaries) as input to its functions. It does not directly read files.
    -   **Output:**
        -   Generates and displays plots using `matplotlib` if the `show` parameter is `True`.
        -   Prints informational messages to the console if `matplotlib` is unavailable or if input data is missing/empty.
        -   Currently, there is no direct functionality to save plots to files, though this could be added via `matplotlib.pyplot.savefig()`.

## 5. Function and Class Example Usages

### [`plot_metrics_dashboard(metrics_history: List[Dict[str, Any]], show: bool = True)`](../../recursive_training/advanced_metrics/visualization.py:23)

-   **Purpose:** Generates and displays a 2x2 dashboard of key metrics (MSE, Accuracy, Mean Uncertainty, Drift Detected, Converged Status) over training iterations.
-   **Conceptual Usage:**
    ```python
    from recursive_training.advanced_metrics.visualization import plot_metrics_dashboard

    metrics_data = [
        {
            "iteration": 1,
            "metrics": {"mse": 0.1, "accuracy": 0.8},
            "advanced_metrics": {
                "uncertainty": {"mean": 0.5},
                "drift": {"detected": False},
                "convergence": {"converged": True}
            }
        },
        {
            "iteration": 2,
            "metrics": {"mse": 0.05, "accuracy": 0.9},
            "advanced_metrics": {
                "uncertainty": {"mean": 0.4},
                "drift": {"detected": True},
                "convergence": {"converged": False}
            }
        }
    ]
    plot_metrics_dashboard(metrics_data, show=True)
    ```

### [`plot_reliability_diagram(calibration_metrics: Dict[str, Any], show: bool = True)`](../../recursive_training/advanced_metrics/visualization.py:84)

-   **Purpose:** Generates and displays a reliability diagram to assess the calibration of a model's probabilistic predictions.
-   **Conceptual Usage:**
    ```python
    from recursive_training.advanced_metrics.visualization import plot_reliability_diagram

    calibration_data = {
        "reliability_diagram": {
            "y_true": [0.0, 0.2, 0.5, 0.7, 1.0], # Observed frequencies
            "y_pred": [0.1, 0.3, 0.4, 0.8, 0.9]  # Predicted probabilities
        }
        # Other calibration metrics like brier_score, ece might also be present
    }
    plot_reliability_diagram(calibration_data, show=True)
    ```

## 6. Hardcoding Issues

-   **Plot Aesthetics:**
    -   Figure sizes are hardcoded: `figsize=(12, 8)` for [`plot_metrics_dashboard()`](../../recursive_training/advanced_metrics/visualization.py:46) and `figsize=(6, 6)` for [`plot_reliability_diagram()`](../../recursive_training/advanced_metrics/visualization.py:107).
    -   Plot colors (e.g., `"tab:blue"`, `"tab:red"`) are hardcoded within [`plot_metrics_dashboard()`](../../recursive_training/advanced_metrics/visualization.py:50).
    -   Titles, labels, and legend entries are hardcoded strings.
-   **Data Access Keys:** The functions rely on specific hardcoded string keys to access data within the input dictionaries (e.g., `"iteration"`, `"metrics"`, `"mse"`, `"advanced_metrics"`, `"uncertainty"`, `"mean"`, `"drift"`, `"detected"`, `"convergence"`, `"converged"`, `"reliability_diagram"`, `"y_true"`, `"y_pred"`). This creates a rigid dependency on the exact structure of the input data.

## 7. Coupling Points

-   **Data Structure Coupling:** The module is tightly coupled to the specific schema of the `metrics_history` list and `calibration_metrics` dictionary. Any changes to these data structures in the modules that produce them would necessitate corresponding changes in this visualization module.
-   **`matplotlib` Dependency:** The core functionality of the module (plotting) is entirely dependent on `matplotlib`. If `matplotlib` is not available, the module's utility is limited to printing informational messages.

## 8. Existing Tests

The test suite in [`tests/recursive_training/advanced_metrics/test_visualization.py`](../../tests/recursive_training/advanced_metrics/test_visualization.py) provides coverage for the module.

-   **Coverage:**
    -   Tests exist for both public functions: [`plot_metrics_dashboard()`](../../recursive_training/advanced_metrics/visualization.py:23) and [`plot_reliability_diagram()`](../../recursive_training/advanced_metrics/visualization.py:84).
-   **Nature of Tests:**
    -   The tests primarily use `unittest.mock.patch` to mock the `matplotlib.pyplot` library.
    -   They verify that `matplotlib` functions (e.g., `plt.show()`, `plt.subplots()`, `plt.figure()`) are called with the expected arguments (like `figsize` or the `show` parameter).
    -   Tests for handling empty input data (e.g., `plot_metrics_dashboard([], show=False)`) are included to ensure graceful failure or no-operation.
    -   Sample data is provided via pytest fixtures ([`metrics_history()`](../../tests/recursive_training/advanced_metrics/test_visualization.py:19) and [`calibration_metrics()`](../../tests/recursive_training/advanced_metrics/test_visualization.py:42)).
-   **Gaps or Problematic Tests:**
    -   **Visual Output Not Verified:** The tests mock out `matplotlib` calls and verify that these calls are made, but they do not (and generally cannot easily) verify the actual visual correctness of the generated plots.
    -   **Data Extraction Logic Not Directly Tested:** While the mock calls imply data is being handled, there are no specific assertions on the intermediate data extracted from the input dictionaries before plotting.
    -   The mocking of `matplotlib.pyplot.axs` in [`test_plot_metrics_dashboard()`](../../tests/recursive_training/advanced_metrics/test_visualization.py:54) and [`test_plot_metrics_dashboard_no_show()`](../../tests/recursive_training/advanced_metrics/test_visualization.py:107) is thorough but verbose, involving mocking each subplot and its methods individually.

## 9. Module Architecture and Flow

-   The module defines two primary public functions for plotting: [`plot_metrics_dashboard()`](../../recursive_training/advanced_metrics/visualization.py:23) and [`plot_reliability_diagram()`](../../recursive_training/advanced_metrics/visualization.py:84).
-   **Initialization:** Global boolean flags ([`MATPLOTLIB_AVAILABLE`](../../recursive_training/advanced_metrics/visualization.py:13) and [`SEABORN_AVAILABLE`](../../recursive_training/advanced_metrics/visualization.py:19)) are set at import time using `try-except ImportError` blocks to check for optional dependencies.
-   **Function Execution Flow:**
    1.  Check if `matplotlib` is available using the global flag. If not, print a message and return.
    2.  Perform basic validation on the input data (e.g., check if `metrics_history` is empty or if `reliability_diagram` data is present). If data is insufficient, print a message and return.
    3.  Extract specific data points from the input dictionaries/lists using `get()` methods for safe access.
    4.  Utilize `matplotlib.pyplot` to create figures (`plt.figure()`, `plt.subplots()`) and axes.
    5.  Plot the extracted data using methods like `axs.plot()` or `plt.plot()`.
    6.  Set plot attributes such as titles, labels, grid, and legends.
    7.  If the `show` parameter is `True` (default), call `plt.show()` to display the plot.

## 10. Naming Conventions

-   **Functions:** Names like [`plot_metrics_dashboard()`](../../recursive_training/advanced_metrics/visualization.py:23) and [`plot_reliability_diagram()`](../../recursive_training/advanced_metrics/visualization.py:84) adhere to PEP 8 (snake_case) and are descriptive.
-   **Variables:** Local variables (e.g., `metrics_history`, `fig`, `axs`, `iterations`, `mse`) generally follow PEP 8 and are clearly named.
-   **Constants:** [`MATPLOTLIB_AVAILABLE`](../../recursive_training/advanced_metrics/visualization.py:13) and [`SEABORN_AVAILABLE`](../../recursive_training/advanced_metrics/visualization.py:19) follow PEP 8 for constants (UPPER_CASE_WITH_UNDERSCORES).
-   **Overall:** Naming is consistent and clear, with no significant deviations from Python community standards or obvious AI-generated naming errors. The chained `get()` calls for accessing nested dictionary keys (e.g., `m.get("advanced_metrics", {}).get("uncertainty", {}).get("mean")`) are a standard Python idiom for safe access.