# Analysis Report: forecast_output.forecast_divergence_detector

**Version:** 1.0.0
**Author:** Pulse AI Engine

## 1. Module Intent/Purpose

The primary role of the [`forecast_output/forecast_divergence_detector.py`](forecast_output/forecast_divergence_detector.py:1) module is to analyze a batch of forecasts and identify symbolic contradictions or divergences. It aims to detect and report on conflicting "arc labels" (or other specified keys) within a given set of forecast data, highlighting potential instability or opposing narratives in the forecast batch.

## 2. Operational Status/Completeness

The module appears to be functionally complete for its defined scope.
- It implements all described functionalities: detecting symbolic opposition, scoring batch divergence, grouping conflicting forecasts, and generating a summary report.
- There are no explicit TODO comments or obvious placeholders indicating unfinished sections.
- Basic input validation (checking if `forecasts` is a list) is present in public functions.

## 3. Implementation Gaps / Unfinished Next Steps

- **Extensibility of Opposition Logic:** The `OPPOSING_ARCS` dictionary is hardcoded. While marked as "Optional: symbolic opposition map", a more robust system might involve loading these oppositions from a configuration file or a database, allowing for easier updates and management without code changes.
- **Advanced Divergence Metrics:** The current `score_batch_divergence` function provides a simple polarization score based on the top two most common labels. More sophisticated divergence metrics could be developed, perhaps considering the entire distribution of labels or the semantic similarity/dissimilarity of non-explicitly opposing arcs.
- **Configurable Key for Forecast Data:** While the `key` parameter (defaulting to `"arc_label"`) allows some flexibility in what part of the forecast dictionary is analyzed, the module assumes a relatively flat structure for the input `forecasts` (list of dictionaries). It might not handle nested structures or more complex forecast objects without modification.
- **Integration with a Larger System:** The module provides tools for analysis but doesn't show how it's integrated into a larger forecast processing pipeline. Logical next steps would involve this module being called by other components that manage forecast batches, and its reports being consumed for decision-making or further analysis.

## 4. Connections & Dependencies

### Direct Imports from Other Project Modules
- None. This module is self-contained in terms of project-specific code dependencies.

### External Library Dependencies
- `typing`: Used for type hinting (`List`, `Dict`, `Tuple`).
- `collections.Counter`: Used in [`score_batch_divergence()`](forecast_output/forecast_divergence_detector.py:40) for counting label occurrences.

### Interaction with Other Modules via Shared Data
- **Input:** The module expects a list of dictionaries (`forecasts: List[Dict]`) as input. This data would presumably be generated by other modules in a forecasting pipeline.
- **Output:**
    - [`detect_symbolic_opposition()`](forecast_output/forecast_divergence_detector.py:27) returns a list of tuples.
    - [`score_batch_divergence()`](forecast_output/forecast_divergence_detector.py:40) returns a float.
    - [`group_conflicting_forecasts()`](forecast_output/forecast_divergence_detector.py:61) returns a dictionary.
    - [`generate_divergence_report()`](forecast_output/forecast_divergence_detector.py:76) returns a dictionary summarizing the divergence.
    - This output data (especially the report) is likely consumed by other modules for logging, alerting, or further decision-making processes.

### Input/Output Files
- The module itself does not directly read from or write to any files (logs, data files, metadata). It operates on in-memory data passed as arguments.

## 5. Function and Class Example Usages

The module primarily consists of functions.

- **`detect_symbolic_opposition(forecasts: List[Dict], key: str = "arc_label") -> List[Tuple[str, str]]`**
    - **Purpose:** Identifies pairs of predefined opposing symbolic labels present in the forecast batch.
    - **Example:**
      ```python
      forecast_batch = [
          {"arc_label": "Hope Surge", "value": 0.8},
          {"arc_label": "Collapse Risk", "value": 0.7},
          {"arc_label": "Stabilization", "value": 0.5}
      ]
      oppositions = detect_symbolic_opposition(forecast_batch)
      # oppositions might be: [("Hope Surge", "Collapse Risk")]
      ```

- **`score_batch_divergence(forecasts: List[Dict], key: str = "arc_label") -> float`**
    - **Purpose:** Calculates a score representing the polarization or narrative opposition in the batch. A higher score (closer to 1.0, though the current formula seems to produce values much lower, max 0.25 for a 50/50 split) indicates greater divergence between the two most prominent labels.
    - **Example:**
      ```python
      forecast_batch = [
          {"arc_label": "Hope Surge"}, {"arc_label": "Hope Surge"},
          {"arc_label": "Collapse Risk"}
      ]
      divergence_score = score_batch_divergence(forecast_batch)
      # divergence_score would be (2/3) * (1/3) = 0.222 (approx)
      ```

- **`group_conflicting_forecasts(forecasts: List[Dict], key: str = "arc_label") -> Dict[str, List[Dict]]`**
    - **Purpose:** Groups forecasts by their symbolic labels if those labels are part of an active opposition pair.
    - **Example:**
      ```python
      forecast_batch = [
          {"id": 1, "arc_label": "Hope Surge"},
          {"id": 2, "arc_label": "Collapse Risk"},
          {"id": 3, "arc_label": "Hope Surge"},
          {"id": 4, "arc_label": "Stabilization"}
      ]
      conflicting_groups = group_conflicting_forecasts(forecast_batch)
      # conflicting_groups might be:
      # {
      #     "Hope Surge": [{"id": 1, "arc_label": "Hope Surge"}, {"id": 3, "arc_label": "Hope Surge"}],
      #     "Collapse Risk": [{"id": 2, "arc_label": "Collapse Risk"}]
      # }
      # (Stabilization would not be included if "Stabilization" vs "X" is not an active conflict)
      ```

- **`generate_divergence_report(forecasts: List[Dict], key: str = "arc_label") -> Dict`**
    - **Purpose:** Consolidates analysis from other functions into a summary report.
    - **Example:**
      ```python
      forecast_batch = [
          {"arc_label": "Hope Surge"},
          {"arc_label": "Collapse Risk"},
          {"arc_label": "Hope Surge"},
      ]
      report = generate_divergence_report(forecast_batch)
      # report would be like:
      # {
      #     "divergence_score": 0.222, # (2/3 * 1/3)
      #     "symbolic_conflicts": [("Hope Surge", "Collapse Risk")],
      #     "cluster_sizes": {"Hope Surge": 2, "Collapse Risk": 1}
      # }
      ```

## 6. Hardcoding Issues

- **`OPPOSING_ARCS` ([`forecast_output/forecast_divergence_detector.py:18-24`](forecast_output/forecast_divergence_detector.py:18))**: This dictionary defines the symbolic oppositions.
    - `{"Hope Surge": "Collapse Risk", "Collapse Risk": "Hope Surge", ...}`
    - This is a primary piece of logic hardcoded into the module. If these relationships change or expand, the code must be modified.
- **Default Key `arc_label`**: Used in all public functions (e.g., [`detect_symbolic_opposition()`](forecast_output/forecast_divergence_detector.py:27)). While configurable via a parameter, its default value is hardcoded.
- **Default "unknown" label**: If the specified `key` is not found in a forecast dictionary, it defaults to `"unknown"` (e.g., in [`detect_symbolic_opposition()`](forecast_output/forecast_divergence_detector.py:36)). This is a hardcoded magic string.
- **Rounding in `score_batch_divergence` ([`forecast_output/forecast_divergence_detector.py:58`](forecast_output/forecast_divergence_detector.py:58))**: The score is rounded to 3 decimal places. This precision is hardcoded.

## 7. Coupling Points

- **Input Data Structure:** The module is tightly coupled to the expected structure of the `forecasts` input: a list of dictionaries, where each dictionary is expected to contain the `key` (e.g., `"arc_label"`). Changes to this data structure in other parts of the system would require updates here.
- **`OPPOSING_ARCS` Definition:** The core logic relies on the hardcoded `OPPOSING_ARCS` map. Any module or system component that defines or relies on these symbolic oppositions must be consistent with this map. This creates a conceptual coupling.

## 8. Existing Tests

- **Internal Test Function:** The module includes a basic test function [`_test_forecast_divergence_detector()`](forecast_output/forecast_divergence_detector.py:94) within an `if __name__ == "__main__":` block.
    - This test uses a small, hardcoded `dummy` forecast batch.
    - It asserts the presence of expected keys in the report and the type of `symbolic_conflicts`.
    - It prints a success message: `"âœ… Forecast divergence detector test passed."`
- **Dedicated Test File:** Based on the `list_files` output for the `tests/` directory, there does **not** appear to be a separate, dedicated test file (e.g., `tests/test_forecast_divergence_detector.py`) for this module.
- **Coverage & Nature:**
    - The internal test provides minimal coverage, checking one specific scenario.
    - It does not cover edge cases, such as:
        - Empty `forecasts` list (though [`score_batch_divergence()`](forecast_output/forecast_divergence_detector.py:50) handles this for its own logic).
        - Forecasts with missing `key`.
        - Scenarios with no oppositions.
        - Scenarios with multiple opposition pairs.
        - Different `key` values.
    - The tests are basic assertion checks and do not use a formal testing framework like `pytest` beyond what might be inferred by the project's overall structure (e.g., `conftest.py` in `tests/`).
- **Gaps:**
    - Lack of a dedicated test file using a standard test runner.
    - Insufficient coverage of edge cases and varied input conditions.
    - No testing for the `ValueError` exceptions raised for invalid input types.

## 9. Module Architecture and Flow

- **Stateless Functions:** The module consists of a set of stateless functions that take data, process it, and return results. There are no classes or persistent state within the module itself.
- **Core Logic - `OPPOSING_ARCS`:** The `OPPOSING_ARCS` dictionary is central to identifying conflicts.
- **Workflow for `generate_divergence_report()`:**
    1. Calls [`detect_symbolic_opposition()`](forecast_output/forecast_divergence_detector.py:27) to find active conflict pairs based on `OPPOSING_ARCS` and the input `forecasts`.
    2. Calls [`score_batch_divergence()`](forecast_output/forecast_divergence_detector.py:40) to calculate a divergence score.
    3. Calls [`group_conflicting_forecasts()`](forecast_output/forecast_divergence_detector.py:61) (which internally calls [`detect_symbolic_opposition()`](forecast_output/forecast_divergence_detector.py:27) again) to group forecasts by their conflicting labels.
    4. Compiles these results into a dictionary.
- **Data Flow:**
    - Input: `List[Dict]` (forecast batch) and an optional `key` string.
    - Intermediate: Sets of labels, counts of labels.
    - Output: Lists of conflict pairs, a divergence score (float), and dictionaries grouping forecasts or summarizing results.

## 10. Naming Conventions

- **Module Name (`forecast_divergence_detector.py`):** Clear and descriptive, follows snake_case.
- **Function Names:**
    - [`detect_symbolic_opposition()`](forecast_output/forecast_divergence_detector.py:27)
    - [`score_batch_divergence()`](forecast_output/forecast_divergence_detector.py:40)
    - [`group_conflicting_forecasts()`](forecast_output/forecast_divergence_detector.py:61)
    - [`generate_divergence_report()`](forecast_output/forecast_divergence_detector.py:76)
    - [`_test_forecast_divergence_detector()`](forecast_output/forecast_divergence_detector.py:94) (private test helper)
    - All are descriptive and follow snake_case (PEP 8).
- **Variable Names:**
    - `forecasts`, `key`, `labels`, `counts`, `total`, `top_two`, `a`, `b`, `active_conflicts`, `group`, `report`, `dummy` are generally clear.
    - `f` is used as a loop variable for individual forecasts, which is common but could be `forecast_item` for extreme clarity in longer loops.
- **Constant Name (`OPPOSING_ARCS`):** Uses `UPPER_SNAKE_CASE`, which is standard for constants (PEP 8).
- **String Literals (arc labels):** e.g., `"Hope Surge"`, `"Collapse Risk"`. These are specific to the domain. Consistency within this set is internal to the `OPPOSING_ARCS` definition.
- **Overall:** Naming conventions appear consistent and largely follow PEP 8. No obvious AI assumption errors in naming are apparent. The names are human-readable and reflect their purpose.